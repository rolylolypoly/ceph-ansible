---
- name: create filesystem pools
  command: "{{ hostvars[groups[mon_group_name][0]]['docker_exec_cmd'] | default('') }} ceph --cluster {{ cluster }} osd pool create {{ item.name }} {{ item.pgs }} {{ item.type | default('') }}"
  changed_when: false
  delegate_to: "{{ groups[mon_group_name][0] }}"
  with_items:
    - "{{ cephfs_pools }}"

- name: configure erasure profile
  command: "ceph osd erasure-code-profile set default plugin=lrc k=1 m=1 l=1 crush-locality=host crush-failure-domain=osd"
  when:
    - cephfs_data.type is defined
    - cephfs_data.type == "erasure"

- name: enable erasure coded pool for cephfs_data
  command: "ceph osd pool set {{ cephfs_data }} allow_ec_overwrites true"
  when:
    - cephfs_data.type is defined
    - cephfs_data.type == "erasure"

- name: enable erasure coded test mode
- block:
  - command: "ceph osd getcrushmap -o ansible_crush_map_compressed"
    args:
      chdir: /root
      creates: /root/ansible_crush_map_compressed
  - command: "crushtool -d crush_map_compressed -o ansible_crush_map_decompressed"
    args:
      chdir: /root
      creates: /root/ansible_crush_map_decompressed
  - replace:
      path: /root/ansible_crush_map_decompressed
      regexp: "step chooseleaf indep 0 type host"
      replace: "step chooseleaf indep 0 type osd"
  - command: "crushtool -c crush_map_decompressed -o ansible_new_crush_map_compressed"
    args:
      chdir: /root
      creates: /root/ansible_new_crush_map_decompressed
  - command: "ceph osd setcrushmap -i ansible_new_crush_map_compressed"
    args:
      chdir: /root
  - command: "ceph os pool set {{ cephfs_data }} min_size 1"
  when:
    - cephfs_data.type is defined
    - cephfs_data.type == "erasure"

- name: check if ceph filesystem already exists
  command: "{{ hostvars[groups[mon_group_name][0]]['docker_exec_cmd'] | default('') }} ceph --cluster {{ cluster }} fs get {{ cephfs }}"
  register: check_existing_cephfs
  changed_when: false
  delegate_to: "{{ groups[mon_group_name][0] }}"
  failed_when: false

- name: create ceph filesystem
  command: "{{ hostvars[groups[mon_group_name][0]]['docker_exec_cmd'] | default('') }} ceph --cluster {{ cluster }} fs new {{ cephfs }} {{ cephfs_metadata }} {{ cephfs_data }}"
  changed_when: false
  delegate_to: "{{ groups[mon_group_name][0] }}"
  when:
    - check_existing_cephfs.rc != 0

- name: assign application to cephfs pools
  command: "{{ hostvars[groups[mon_group_name][0]]['docker_exec_cmd'] | default('') }} ceph --cluster {{ cluster }} osd pool application enable {{ item }} {{ cephfs }}"
  with_items:
    - "{{ cephfs_data }}"
    - "{{ cephfs_metadata }}"
  changed_when: false
  delegate_to: "{{ groups[mon_group_name][0] }}"
  when:
    - check_existing_cephfs.rc != 0
    - ceph_release_num[ceph_release] >= ceph_release_num['luminous']

- name: allow multimds
  command: "{{ hostvars[groups[mon_group_name][0]]['docker_exec_cmd'] | default('') }} ceph --cluster {{ cluster }} fs set {{ cephfs }} allow_multimds true --yes-i-really-mean-it"
  changed_when: false
  delegate_to: "{{ groups[mon_group_name][0] }}"
  when:
    - ceph_release_num[ceph_release] == ceph_release_num.luminous

- name: set max_mds
  command: "{{ hostvars[groups[mon_group_name][0]]['docker_exec_cmd'] | default('') }} ceph --cluster {{ cluster }} fs set {{ cephfs }} max_mds {{ mds_max_mds }}"
  changed_when: false
  delegate_to: "{{ groups[mon_group_name][0] }}"
  when:
    - ceph_release_num[ceph_release] >= ceph_release_num.jewel
    - mds_max_mds > 1
